# Website Data Documentation

This folder contains all JSON data files used by the Singapore Budget Speeches website. This document explains each dataset, its source, and how to regenerate it.

## Directory Structure

```
docs/data/
├── summary/                    # Lightweight overview files (loaded on page init)
│   ├── ministries_overview.json
│   ├── ministers_overview.json
│   ├── yearly_overview.json
│   ├── global_overview.json
│   ├── global_time_series.json
│   ├── global_country_details.json
│   └── global_map_data.json
├── search-index/               # Sharded search data (loaded on demand)
│   ├── overview.json
│   ├── decades/
│   │   ├── 1960s.json
│   │   ├── 1970s.json
│   │   └── ...
│   └── topics/
└── detailed/                   # Detailed per-year data (reserved for future use)
```

---

## Data Sources

All data is derived from:

1. **Parquet files** in `output_processor/` - Processed sentence-level data from budget speeches
2. **CSV files** in `analysis/` - Aggregated statistics from analysis notebooks

---

## Regenerating Data

### 1. Topics, Ministers, Search Index

**Source script:** `analysis/export_for_web.py`

**Input files (CSV):**

- `analysis/ministry_by_year.csv`
- `analysis/minister_speech_statistics.csv`
- `analysis/yearly_speech_statistics.csv`

**Output files:**

- `docs/data/summary/ministries_overview.json`
- `docs/data/summary/ministers_overview.json`
- `docs/data/summary/yearly_overview.json`
- `docs/data/search-index/overview.json`
- `docs/data/search-index/decades/*.json`

**To regenerate:**

```bash
poetry run python analysis/export_for_web.py
```

### 2. Global References (Country Mentions)

**Source script:** `analysis/country_extraction.py`

**Input files:**

- `output_processor/*.parquet` (66 parquet files, one per year)

**Output files:**

- `docs/data/summary/global_overview.json` - Total mentions, regional breakdown
- `docs/data/summary/global_time_series.json` - Yearly counts for top 20 countries
- `docs/data/summary/global_country_details.json` - Actual quotes with context
- `docs/data/summary/global_map_data.json` - ISO codes for choropleth map

**To regenerate:**

```bash
poetry run python analysis/country_extraction.py
```

### 3. Linguistic Features (Advanced NLP)

**Source script:** `analysis/linguistic_features.py`

**Input files:**

- `output_processor/*.parquet` (66 parquet files, one per year)

**Output files:**

- `analysis/linguistic_features.csv` - Raw linguistic metrics per year
- Data is incorporated into `docs/data/summary/yearly_overview.json` via `export_for_web.py`

**Metrics calculated:**

| Metric | Library | Description |
|--------|---------|-------------|
| Type-Token Ratio (TTR) | `lexicalrichness` | Vocabulary diversity (unique words / total words) |
| MTLD | `lexicalrichness` | Measure of Textual Lexical Diversity (length-independent) |
| Temporal Orientation | keyword matching | Forward vs backward-looking language ratio |
| Certainty Index | keyword matching | Confident vs hedging language ratio |
| Passive Voice Ratio | `spacy` | Proportion of passive voice constructions |

**To regenerate:**

```bash
# Step 1: Generate linguistic features CSV
poetry run python analysis/linguistic_features.py

# Step 2: Export to JSON (includes linguistic data in yearly_overview.json)
poetry run python analysis/export_for_web.py
```

---

## File Descriptions

### Summary Files

| File                          | Generated By                    | Description                              | Used By        |
| ----------------------------- | ------------------------------- | ---------------------------------------- | -------------- |
| `ministries_overview.json`    | `export_for_web.py`             | Topic coverage % by year, topic totals   | Topics page    |
| `ministers_overview.json`     | `export_for_web.py`             | Minister stats (tenure, word counts)     | Ministers page |
| `yearly_overview.json`        | `export_for_web.py`             | Per-year metrics (sentences, readability)| Home, Language |
| `global_overview.json`        | `country_extraction.py`         | Country mention totals, regional breakdown | Global page  |
| `global_time_series.json`     | `country_extraction.py`         | Yearly counts for top 20 countries       | Global page    |
| `global_country_details.json` | `country_extraction.py`         | Quotes mentioning each country           | Global page    |
| `global_map_data.json`        | `country_extraction.py`         | ISO-3 codes with counts for choropleth   | Global page    |

### Search Index Files

| File                 | Generated By        | Description                              |
| -------------------- | ------------------- | ---------------------------------------- |
| `overview.json`      | `export_for_web.py` | Search index metadata                    |
| `decades/1960s.json` | `export_for_web.py` | All sentences from 1960-1969 with topics |
| `decades/1970s.json` | `export_for_web.py` | All sentences from 1970-1979 with topics |
| ...                  | `export_for_web.py` | (one file per decade)                    |
| `topics/*.json`      | `export_for_web.py` | Sentences grouped by topic (if generated)|

---

## When to Regenerate

| Scenario                          | Action Required                                                |
| --------------------------------- | -------------------------------------------------------------- |
| New budget speech added           | Run `linguistic_features.py`, `export_for_web.py`, `country_extraction.py` |
| Analysis CSVs updated             | Run `export_for_web.py`                                        |
| Country aliases changed           | Run `country_extraction.py`                                    |
| New topic classification rules    | Run `export_for_web.py`                                        |
| Linguistic analysis updated       | Run `linguistic_features.py` then `export_for_web.py`          |

---

## Adding a New Year's Data

When a new budget speech is added (e.g., 2026):

1. **Extract speech** - Run `extractor/main.py` to get markdown
2. **Process sentences** - Run `processor/main.py` to generate parquet
3. **Update analysis CSVs** - Re-run analysis notebooks
4. **Regenerate web data:**

```bash
# Regenerate linguistic features
poetry run python analysis/linguistic_features.py

# Regenerate topics, ministers, search index (includes linguistic data)
poetry run python analysis/export_for_web.py

# Regenerate global references
poetry run python analysis/country_extraction.py
```

5. **Verify** - Check the website locally before committing

---

## Methodology Notes

### Topic Classification

Topics are assigned using keyword pattern matching. See `analysis/ministry_topic_analysis.ipynb` for the classification rules.

### Global References

Country mentions are identified using:

- Official names: "United States", "United Kingdom"
- Abbreviations: "US", "USA", "UK", "UAE"
- Demonyms: "American", "British", "Chinese"
- Major cities: "Tokyo", "London", "Beijing"
- Historical names: "Malaya", "Burma", "Ceylon", "Soviet Union"

Word boundary matching (`\b`) prevents false positives. See `analysis/country_extraction.py` for the full country list with aliases.

---

## Troubleshooting

**"File not found" errors when running scripts:**

- Ensure you're in the project root directory
- Check that analysis CSVs exist in `analysis/`
- Verify parquet files exist in `output_processor/`

**Data appears stale:**

- Check file modification dates
- Re-run the appropriate generation script
- Clear browser cache when testing

**Missing country in global data:**

- Add the country and aliases to `COUNTRIES` dict in `country_extraction.py`
- Re-run the script
